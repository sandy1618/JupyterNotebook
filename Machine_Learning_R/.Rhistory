{r install-tidyverse, eval = F}
install.packages("tidyverse", repos = 'https://cran.us.r-project.org')
install.packages(c("acepack", "akima", "AmesHousing", "animation", "anomalyDetection", "anytime", "arules", "aspace", "astsa", "avail-able", "bagRboostR", "baseline", "bcpa", "bda", "birk", "bit64", "blogdown", "bookdown", "bookdownplus", "Boom-SpikeSlab", "boot", "breakDown", "breakpoint", "brms", "broom", "bsts", "C50", "car", "caret", "caretEnsemble", "CausalImpactchangepoint", "changepoint.np", "ChemoSpec", "cgwtools", "class", "ClimClass", "cluster.datasets", "CORE-learn", "corrgram", "cowplot", "cpca", "ctv", "CVXR", "data.table", "data.tree", "dataMaid", "DBI", "DBItest", "dbscanddiv", "devtools", "DiagrammeR", "DiagrammeRsvg", "dice", "digest", "doParallel", "dplyr", "dummies", "dtwcluste1071", "eemR", "ElemStatLearn", "factoextra", "fastcluster", "feather", "flexclust", "forecast", "foreach", "gam", "gap-minder", "gbm", "gclus", "GGally", "gganimate", "ggbiplot", "ggmap", "ggplot2", "ggpubr", "ggQC", "ggRandomFor-est", "ggraph", "ggridges", "ggthemes", "ggvis", "glmnet", "gmodels", "googleVis", "gridBase", "gridExtra", "gsl", "gstatgWidgets", "h2o", "HadoopStreaming", "HarmonicRegression", "hcp", "hdpca", "hexbin", "HH", "httr", "htmlwidgetshyperSpec", "igraph", "infer", "ipred", "IQCC", "ISLR", "itertools", "jsonlite", "kableExtra", "keras", "kerasR", "kernlabkeyring", "kgc", "klaR", "knitcitations", "knitr", "Lahman", "lars", "lavaan", "lavaan.survey", "leaps", "learningr", "learNNlearnBayes", "lime", "lme4", "lobstr", "logitnorm", "magick", "magrittr", "Make", "mapdata", "Mapmate", "mapprojmaps", "maptools", "MASS", "Matrix", "MatrixModels", "matrixStats", "markovchain", "mcmc", "MCMCglmmmetRology", "Metrics", "mgcv", "minpack.lm", "MTS", "multiway", "NbClust", "netSEM", "neural", "neuralnet", "Neu-ralNetTools", "nnet", "nycflights13", "odbc", "OIdata", "olsrr", "OIsurv", "onehot", "onlineCPD", "openintro", "optimxpackrat", "pacman", "parallelSVM", "pca3d", "PerformanceAnalytics", "pipeR", "plot3D", "plotmo", "plotKM"))
library("tidyverse")
mpg
?mpg
ggplot(data=mpg) + geom_point(mapping = aes(x=hwy,y=cyl))
ggplot(data=mpg) + geom_point(mapping = aes(x=class,y=drv))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = class))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, size = class))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, shape = class))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = "blue"))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = class)
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy), color = "blue")
?mpg
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = cty))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color = cty,shape=cty))
ggplot(data = mpg) +
geom_point(mapping = aes(x = displ, y = hwy, color =class,shape=class))
?geom_point
gggplot(data=mpg)+
ggplot(data=mpg)+
geom_smooth(mapping=aes(x=displ,y=hwy))
x <- 1
print(x)
1
msg
msg<-"hell9"
print(msg)
x<-1
x
x<-10:30
x
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +
geom_smooth(mapping = aes(color = NULL), se = FALSE)
geom_point()
ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) +
geom_smooth(mapping = aes(group = 123), se = FALSE) +
geom_point()
library(ISLR)
library(ISLR)
?ISLR
?sample
library(ISLR)
set.seed(1)
train = sample(392,198)
library(ISLR)
set.seed(1)
train = sample(392,198)
train
library(ISLR)
set.seed(1)
train = sample(392,300)
train
library(ISLR)
set.seed(1)
train = sample(392,196)
train
knitr::opts_chunk$set(echo = TRUE)
head(cars)
scatter.smooth(x=cars$speed, y=cars$dist, main="Dist ~ Speed")  # scatterplot
scatter.smooth(x=cars$speed, y=cars$dist, main="Dist ~ Speed")  # scatterplot
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(echo = TRUE)
cor(cars$speed, cars$dist)  # calculate correlation between speed and distance
linearMod <- lm(dist ~ speed, data=cars)  # build linear regression model on full data
print(linearMod)
lm <- lm(dist ~ speed, data=cars)  # build linear regression model on full data
print(lm)
cor(cars$speed, cars$dist)  # calculate correlation between speed and distance
summary(lm)
AIC(lm)
AIC(lm)
AIC(lm)
# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
# Build the model on training data -
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance
summary (lmMod)
# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
# Build the model on training data -
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance
summary (lmMod)
AIC (lmMod)
library(DAAG)
install.packages("DAAG")
library(DAAG)
cvResults <- suppressWarnings(CVlm(df=cars, form.lm=dist ~ speed, m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."));  # performs the CV
library(DAAG)
cvResults <- suppressWarnings(CVlm(df = cars, form.lm=dist ~ speed, m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."));  # performs the CV
library(DAAG)
cvResults <- suppressWarnings(CVlm(df = car, form.lm=dist ~ speed, m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."));  # performs the CV
library(DAAG)
cvResults <- suppressWarnings(CVlm(df = cars, form.lm=dist ~ speed, m=5, dots=FALSE, seed=29, legend.pos="topleft",  printit=FALSE, main="Small symbols are predicted values while bigger ones are actuals."));  # performs the CV
predict(lmMod, data.frame(lstat = c(5,10,15)))
plot(dist~speed, data = trainingData)
predict(lmMod, data.frame(lstat = c(5,10,15)))
plot(dist~speed, data = trainingData)
predict(lmMod, data.frame(lstat = c(5,10,15)))
plot(dist~speed, data = trainingData)
# predict(lmMod, data.frame(lstat = c(5,10,15)))
plot(dist~speed, data = trainingData)
abline(lmMod, col = 'red')
# predict(lmMod, data.frame(lstat = c(5,10,15)))
plot(dist~speed, data = trainingData)
abline(lmMod, col = 'red')
abline(lm, col = 'blue')
# predict(lmMod, data.frame(lstat = c(5,10,15)))
# Create Training and Test data -
set.seed(100)  # setting seed to reproduce results of random sampling
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars))  # row indices for training data
trainingData <- cars[trainingRowIndex, ]  # model training data
testData  <- cars[-trainingRowIndex, ]   # test data
# Build the model on training data -
lmMod <- lm(dist ~ speed, data=trainingData)  # build the model
distPred <- predict(lmMod, testData)  # predict distance
summary (lmMod)
AIC (lmMod)
summary(lm)
getwd()
setwd(/home/sandy/Educational/Programming_blog/Machine_Learning_R
)
setwd("/home/sandy/Educational/Programming_blog/Machine_Learning_R")
getwd()
setwd("/home/sandy/Educational/Programming_blog/Machine_Learning_R")
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c("S"))
training.data.raw
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
sapply(training.data.raw,function(x) sum(is.na(x)))
sapply(training.data.raw, function(x) length(unique(x)))
library(Amelia)
missmap(training.data.raw, main = "Missing values vs observed")
install.packages("Amelia")
library(Amelia)
library(Amelia)
install.packages("Amelia")
install.packages("RcppArmadillo")
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data
data <- data[!is.na(data$Embarked),]
data
data
data
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Sex)
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data2 <- data[!is.na(data$Embarked),]
data2
data
rownames(data) <- NULL
data2
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
sapply(training.data.raw,function(x) sum(is.na(x)))
sapply(training.data.raw, function(x) length(unique(x)))
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Sex)
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data <- data[!is.na(data$Embarked),]
data
rownames(data) <- NULL
data
#Model fitting
train <- data[1:800,]
test <- data[801:889,]
model <- glm(Survived ~.,family=binomial(link='logit'),data=train)
summary(model)
# Data Cleaning
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
sapply(training.data.raw,function(x) sum(is.na(x)))
sapply(training.data.raw, function(x) length(unique(x)))
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Sex)
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data <- data[!is.na(data$Embarked),]
data
rownames(data) <- NULL
data
#Model fitting
train <- data[1:800,]
test <- data[801:889,]
model <- glm(Survived ~.,family=binomial(link='logit'),data=train)
summary(model)
# Model Prediction
fitted.results <- predict(model,newdata=subset(test,select=c(2,3,4,5,6,7,8)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))
"
print(paste('Accuracy',1-misClasificError))
# Data Cleaning
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
sapply(training.data.raw,function(x) sum(is.na(x)))
sapply(training.data.raw, function(x) length(unique(x)))
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Sex)
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data <- data[!is.na(data$Embarked),]
data
rownames(data) <- NULL
data
#Model fitting
train <- data[1:800,]
test <- data[801:889,]
model <- glm(Survived ~.,family=binomial(link='logit'),data=train)
summary(model)
# Model Prediction
fitted.results <- predict(model,newdata=subset(test,select=c(2,3,4,5,6,7,8)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))
# Performance Measurement of a Binary Classifier.
library(ROCR)
install.packages("ROCR")
# Data Cleaning
training.data.raw <- read.csv('./titanic/train.csv',header=T,na.strings=c(""))
training.data.raw
sapply(training.data.raw,function(x) sum(is.na(x)))
sapply(training.data.raw, function(x) length(unique(x)))
data <- subset(training.data.raw,select=c(2,3,5,6,7,8,10,12))
data
data$Age[is.na(data$Age)] <- mean(data$Age,na.rm=T)
data
is.factor(data$Sex)
is.factor(data$Embarked)
contrasts(data$Sex)
contrasts(data$Embarked)
data
data <- data[!is.na(data$Embarked),]
data
rownames(data) <- NULL
data
#Model fitting
train <- data[1:800,]
test <- data[801:889,]
model <- glm(Survived ~.,family=binomial(link='logit'),data=train)
summary(model)
# Model Prediction
fitted.results <- predict(model,newdata=subset(test,select=c(2,3,4,5,6,7,8)),type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test$Survived)
print(paste('Accuracy',1-misClasificError))
# Performance Measurement of a Binary Classifier.
library(ROCR)
p <- predict(model, newdata=subset(test,select=c(2,3,4,5,6,7,8)), type="response")
pr <- prediction(p, test$Survived)
prf <- performance(pr, measure = "tpr", x.measure = "fpr")
plot(prf)
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
auc <- performance(pr, measure = "auc")
auc <- auc@y.values[[1]]
auc
plot(prf)
